start: io_goal
nodes:
  io_goal:
    question: >
      I/O performance issues usually relate to
      storage hardware limits, scheduler behavior,
      filesystem configuration, or background workloads.
      What problem best matches your situation?
    options:
      s: "Disk access feels slow"
      h: "High I/O wait (system feels stuck)"
      t: "Tune or understand the I/O scheduler"
    next:
      s: slow_intro
      h: iowait_intro
      t: scheduler_intro
  slow_intro:
    question: >
      Performance characteristics depend heavily
      on the storage device type.
      What storage are you using?
    options:
      n: "NVMe SSD"
      s: "SATA SSD"
      h: "Mechanical HDD"
    next:
      n: nvme
      s: sata_ssd
      h: hdd
  nvme:
    question: >
      NVMe drives are extremely fast.
      Is performance still unexpectedly slow?
    options:
      y: "Yes"
      n: "No"
    next:
      y: nvme_check
      n: nvme_ok
  nvme_ok:
    result:
      - description: >
          NVMe devices already operate near optimal limits.
          Further tuning rarely improves real-world performance.
          Focus instead on application behavior and filesystem choice.
  nvme_check:
    result:
      - description: >
          Check NVMe device detection and health.
        command: "nvme list"
      - description: >
          Ensure the PCIe link is not downgraded in BIOS or firmware.
      - description: >
          Thermal throttling can significantly reduce NVMe speed.
          Verify cooling and airflow.
  sata_ssd:
    question: >
      SATA SSDs benefit from maintenance features.
      Is TRIM enabled?
    options:
      y: "Yes"
      n: "No"
    next:
      y: sata_trim_ok
      n: sata_trim_enable
  sata_trim_ok:
    result:
      - description: >
          TRIM is enabled.
          Performance should remain stable over time.
  sata_trim_enable:
    result:
      - description: >
          Enable periodic TRIM using the systemd timer.
        command: "systemctl enable --now fstrim.timer"
      - description: >
          This is recommended for all SSDs.
          Continuous mount-option TRIM is not required.
  hdd:
    question: >
      Mechanical disks are limited by seek time.
      Are you experiencing frequent random access?
    options:
      y: "Yes"
      n: "Mostly sequential access"
    next:
      y: hdd_seek
      n: hdd_seq
  hdd_seek:
    result:
      - description: >
          HDDs perform poorly with random I/O.
          This is a hardware limitation.
          Consider:
          - Reducing background services
          - Migrating workloads to SSD storage
  hdd_seq:
    result:
      - description: >
          Sequential access is the best-case workload for HDDs.
          Performance is likely within expected limits.
  iowait_intro:
    question: >
      High I/O wait means the CPU is idle
      while waiting for disk operations.
      Have you identified the process causing it?
    options:
      y: "Yes"
      n: "No"
    next:
      y: iowait_known
      n: iowait_identify
  iowait_identify:
    result:
      - description: >
          Identify I/O-heavy processes interactively.
        command: "iotop"
      - description: >
          Alternative overview of system activity.
        command: "htop"
      - description: >
          Look for:
          - Database workloads
          - Package builds
          - Log-heavy services
          - Backup or indexing jobs
  iowait_known:
    question: >
      Is the workload expected (e.g. backup, compile)?
    options:
      y: "Yes"
      n: "No"
    next:
      y: expected_iowait
      n: unexpected_iowait
  expected_iowait:
    result:
      - description: >
          High I/O wait is normal for heavy disk workloads.
          No tuning is required unless it affects usability.
          Use nice or ionice to reduce impact.
  unexpected_iowait:
    result:
      - description: >
          Unexpected I/O wait may indicate:
          - Failing disk
          - Swap thrashing
          - Misbehaving service
          Check disk health using SMART tools.
  scheduler_intro:
    question: >
      The I/O scheduler decides how disk requests
      are ordered and merged.
      Why are you considering changing it?
    options:
      p: "Poor performance"
      l: "Low latency tuning"
      c: "Just curious"
    next:
      p: scheduler_perf
      l: scheduler_latency
      c: scheduler_info
  scheduler_info:
    result:
      - description: >
          Modern Linux defaults are already optimized.
          Manual scheduler tuning is rarely necessary.
  scheduler_perf:
    question: >
      What type of storage is this for?
    options:
      n: "NVMe"
      s: "SSD"
      h: "HDD"
    next:
      n: sched_none
      s: sched_ssd
      h: sched_hdd
  sched_none:
    result:
      - description: >
          NVMe devices typically use the `none` scheduler.
          The device handles scheduling internally.
          Changing schedulers provides no benefit.
  sched_ssd:
    result:
      - description: >
          Recommended schedulers for SSDs:
          - mq-deadline (good default)
          - none (sometimes acceptable)
          Avoid BFQ unless you understand its tradeoffs.
  sched_hdd:
    result:
      - description: >
          HDDs benefit from request merging.
          Recommended scheduler:
          - mq-deadline
          BFQ may improve interactivity on very slow disks.
  scheduler_latency:
    result:
      - description: >
          Low-latency tuning is mostly relevant for HDDs.
          BFQ can improve desktop responsiveness
          at the cost of throughput.
          Not recommended for servers or SSDs.
  finish_io:
    result:
      - description: >
          Key takeaways:
          - Hardware limits matter most
          - SSDs need TRIM, not tuning
          - High I/O wait is often workload-related
          - Scheduler defaults are usually optimal
